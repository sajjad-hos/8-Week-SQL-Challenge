## Case Study #5 - Data Mart - 2. Data Exploration | Solutions | Result
#### Q1: What day of the week is used for each week_date value?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT DISTINCT(TO_CHAR(week_date, 'day')) AS day_of_the_week 
FROM data_mart.clean_weekly_sales;
  ````

#### ðŸ“Š Query Result & Insights:
| day_of_the_week |
| --------------- |
| monday          |

- Monday was used for each week_date value.
---
#### Q2: What range of week numbers are missing from the dataset?
#### ðŸ§  My Approach & Solution:
- Used generate_series(1, 52) to create a list of week numbers from 1st week to the 52nd week .
- Used LEFT JOIN between clean_weekly_sales and all_weeks tables to check which week numbers exist.
- Used HAVING COUNT(cs.week_number) = 0 to filter out weeks that donâ€™t exist in the data.

Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
WITH all_weeks AS (SELECT generate_series(1, 52) AS week_number)
-- Compare with the dataset to find missing week numbers
SELECT aw.week_number
FROM all_weeks aw
LEFT JOIN data_mart.clean_weekly_sales cs ON aw.week_number = cs.week_number
GROUP BY aw.week_number
HAVING COUNT(cs.week_number) = 0
ORDER BY aw.week_number;
  ````

#### ðŸ“Š Query Result & Insights:
| week_number |
| ----------- |
| 1           |
| 2           |
| 3           |
| 4           |
| 5           |
| 6           |
| 7           |
| 8           |
| 9           |
| 10          |
| 11          |
| 12          |
| 37          |
| 38          |
| 39          |
| 40          |
| 41          |
| 42          |
| 43          |
| 44          |
| 45          |
| 46          |
| 47          |
| 48          |
| 49          |
| 50          |
| 51          |
| 52          |

- The clean_weekly_sales dataset is missing weeks 1â€“10 and 37â€“52, for a total of 28 weeks.
---
#### Q3: How many total transactions were there for each year in the dataset?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT calendar_year, SUM(transactions) AS total_transactions
FROM data_mart.clean_weekly_sales
GROUP BY calendar_year
ORDER BY calendar_year;
  ````

#### ðŸ“Š Query Result & Insights:
| calendar_year | total_transactions |
| ------------- | ------------------ |
| 2018          | 346406460          |
| 2019          | 365639285          |
| 2020          | 375813651          |

---
#### Q4: What is the total sales for each region for each month?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT 
  region, month_number,
  SUM(sales) AS total_sales
FROM data_mart.clean_weekly_sales
GROUP BY region, month_number
ORDER BY region, month_number;
  ````

#### ðŸ“Š Query Result & Insights:
| region        | month_number | total_sales |
| ------------- | ------------ | ----------- |
| AFRICA        | 3            | 567767480   |
| AFRICA        | 4            | 1911783504  |
| AFRICA        | 5            | 1647244738  |
| AFRICA        | 6            | 1767559760  |
| AFRICA        | 7            | 1960219710  |
| AFRICA        | 8            | 1809596890  |
| AFRICA        | 9            | 276320987   |
| ASIA          | 3            | 529770793   |
| ASIA          | 4            | 1804628707  |
| ASIA          | 5            | 1526285399  |
| ASIA          | 6            | 1619482889  |
| ASIA          | 7            | 1768844756  |
| ASIA          | 8            | 1663320609  |
| ASIA          | 9            | 252836807   |
| CANADA        | 3            | 144634329   |
| CANADA        | 4            | 484552594   |
| CANADA        | 5            | 412378365   |
| CANADA        | 6            | 443846698   |
| CANADA        | 7            | 477134947   |
| CANADA        | 8            | 447073019   |
| CANADA        | 9            | 69067959    |
| EUROPE        | 3            | 35337093    |
| EUROPE        | 4            | 127334255   |
| EUROPE        | 5            | 109338389   |
| EUROPE        | 6            | 122813826   |
| EUROPE        | 7            | 136757466   |
| EUROPE        | 8            | 122102995   |
| EUROPE        | 9            | 18877433    |
| OCEANIA       | 3            | 783282888   |
| OCEANIA       | 4            | 2599767620  |
| OCEANIA       | 5            | 2215657304  |
| OCEANIA       | 6            | 2371884744  |
| OCEANIA       | 7            | 2563459400  |
| OCEANIA       | 8            | 2432313652  |
| OCEANIA       | 9            | 372465518   |
| SOUTH AMERICA | 3            | 71023109    |
| SOUTH AMERICA | 4            | 238451531   |
| SOUTH AMERICA | 5            | 201391809   |
| SOUTH AMERICA | 6            | 218247455   |
| SOUTH AMERICA | 7            | 235582776   |
| SOUTH AMERICA | 8            | 221166052   |
| SOUTH AMERICA | 9            | 34175583    |
| USA           | 3            | 225353043   |
| USA           | 4            | 759786323   |
| USA           | 5            | 655967121   |
| USA           | 6            | 703878990   |
| USA           | 7            | 760331754   |
| USA           | 8            | 712002790   |
| USA           | 9            | 110532368   |

---
#### Q5: What is the total count of transactions for each platform?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT 
	  platform,
      SUM(transactions) AS total_transactions
FROM data_mart.clean_weekly_sales
GROUP BY platform
ORDER BY total_transactions DESC;
  ````

#### ðŸ“Š Query Result & Insights:
| platform | total_transactions |
| -------- | ------------------ |
| Retail   | 1081934227         |
| Shopify  | 5925169            |

---
#### Q6: What is the percentage of sales for Retail vs Shopify for each month?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT
    calendar_year,
    month_number,
    ROUND(100.0 * SUM(CASE WHEN platform = 'Retail' THEN sales ELSE 0 END) 
          / SUM(sales), 2) AS retail_percentage,
    ROUND(100.0 * SUM(CASE WHEN platform = 'Shopify' THEN sales ELSE 0 END) 
          / SUM(sales), 2) AS shopify_percentage
FROM data_mart.clean_weekly_sales
GROUP BY calendar_year, month_number
ORDER BY calendar_year, month_number;
  ````

#### ðŸ“Š Query Result & Insights:
| calendar_year | month_number | retail_percentage | shopify_percentage |
| ------------- | ------------ | ----------------- | ------------------ |
| 2018          | 3            | 97.92             | 2.08               |
| 2018          | 4            | 97.93             | 2.07               |
| 2018          | 5            | 97.73             | 2.27               |
| 2018          | 6            | 97.76             | 2.24               |
| 2018          | 7            | 97.75             | 2.25               |
| 2018          | 8            | 97.71             | 2.29               |
| 2018          | 9            | 97.68             | 2.32               |
| 2019          | 3            | 97.71             | 2.29               |
| 2019          | 4            | 97.80             | 2.20               |
| 2019          | 5            | 97.52             | 2.48               |
| 2019          | 6            | 97.42             | 2.58               |
| 2019          | 7            | 97.35             | 2.65               |
| 2019          | 8            | 97.21             | 2.79               |
| 2019          | 9            | 97.09             | 2.91               |
| 2020          | 3            | 97.30             | 2.70               |
| 2020          | 4            | 96.96             | 3.04               |
| 2020          | 5            | 96.71             | 3.29               |
| 2020          | 6            | 96.80             | 3.20               |
| 2020          | 7            | 96.67             | 3.33               |
| 2020          | 8            | 96.51             | 3.49               |

---
#### Q7: What is the percentage of sales by demographic for each year in the dataset?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT
    calendar_year,
    ROUND(100.0 * SUM(CASE WHEN demographic = 'Couples' THEN sales ELSE 0 END) / SUM(sales), 2) AS couples_percentage,
    ROUND(100.0 * SUM(CASE WHEN demographic = 'Families' THEN sales ELSE 0 END) / SUM(sales), 2) AS families_percentage,
    ROUND(100.0 * SUM(CASE WHEN demographic = 'unknown' THEN sales ELSE 0 END) / SUM(sales), 2) AS unknown_percentage
FROM data_mart.clean_weekly_sales
GROUP BY calendar_year
ORDER BY calendar_year;
  ````

#### ðŸ“Š Query Result & Insights:
| calendar_year | couples_percentage | families_percentage | unknown_percentage |
| ------------- | ------------------ | ------------------- | ------------------ |
| 2018          | 26.38              | 31.99               | 41.63              |
| 2019          | 27.28              | 32.47               | 40.25              |
| 2020          | 28.72              | 32.73               | 38.55              |

---
#### Q8: Which age_band and demographic values contribute the most to Retail sales?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT 
    age_band, 
    demographic, 
    SUM(sales) AS retail_sales,
    ROUND(100.0 * SUM(sales) / SUM(SUM(sales)) OVER (), 1) AS contribution_percentage
FROM data_mart.clean_weekly_sales
WHERE platform = 'Retail'
GROUP BY age_band, demographic
ORDER BY retail_sales DESC;
  ````

#### ðŸ“Š Query Result & Insights:
| age_band     | demographic | retail_sales | contribution_percentage |
| ------------ | ----------- | ------------ | ----------------------- |
| unknown      | unknown     | 16067285533  | 40.5                    |
| Retirees     | Families    | 6634686916   | 16.7                    |
| Retirees     | Couples     | 6370580014   | 16.1                    |
| Middle Aged  | Families    | 4354091554   | 11.0                    |
| Young Adults | Couples     | 2602922797   | 6.6                     |
| Middle Aged  | Couples     | 1854160330   | 4.7                     |
| Young Adults | Families    | 1770889293   | 4.5                     |

---
#### Q9: Can we use the avg_transaction column to find the average transaction size for each year for Retail vs Shopify? If not - how would you calculate it instead?
#### ðŸ§  My Approach & Solution:
Note: Use the `code` from `Data Cleansing Steps` to create the `clean_weekly_sales` table first. Then, run the queries below to get the results.
````sql
SELECT 
  calendar_year, 
  platform, 
  ROUND(AVG(avg_transaction),0) AS avg_transaction_row, 
  ROUND(SUM(sales)::NUMERIC / SUM(transactions), 1) AS avg_transaction_group
FROM data_mart.clean_weekly_sales
GROUP BY calendar_year, platform
ORDER BY calendar_year, platform;
  ````

#### ðŸ“Š Query Result & Insights:
| calendar_year | platform | avg_transaction_row | avg_transaction_group |
| ------------- | -------- | ------------------- | --------------------- |
| 2018          | Retail   | 43                  | 36.6                  |
| 2018          | Shopify  | 188                 | 192.5                 |
| 2019          | Retail   | 42                  | 36.8                  |
| 2019          | Shopify  | 178                 | 183.4                 |
| 2020          | Retail   | 41                  | 36.6                  |
| 2020          | Shopify  | 175                 | 179.0                 |

- The avg_transaction column shows the average per row, and each row can have a different number of transactions. Simply averaging these values treats all rows equally, which can give an inaccurate result. The correct approach is to calculate the average transaction size at the group level by dividing total sales by total transactions for each year and platform.
---
